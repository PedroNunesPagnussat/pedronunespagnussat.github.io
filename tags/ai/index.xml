<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>AI on Pedro Nunes Pagnussat</title>
    <link>http://localhost:1313/tags/ai/</link>
    <description>Recent content in AI on Pedro Nunes Pagnussat</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 31 Mar 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to Run Local LLMs with Ollama</title>
      <link>http://localhost:1313/blogs/local_llm_with_ollama/</link>
      <pubDate>Mon, 31 Mar 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/blogs/local_llm_with_ollama/</guid>
      <description>&lt;p&gt;Running local LLMs with Ollama is an exciting way to bring powerful AI models directly on your hardware, enhancing privacy, network dependancy, and reducing development costs.&lt;/p&gt;&#xA;&lt;h2 id=&#34;why-run-local-llms&#34;&gt;Why Run Local LLMs?&lt;/h2&gt;&#xA;&lt;ul&gt;&#xA;&lt;li&gt;&lt;strong&gt;Privacy:&lt;/strong&gt; Your data stays entirely on your local machine, crucial for sensitive information.&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Speed and Availability:&lt;/strong&gt; No network latency or Availability problems&lt;/li&gt;&#xA;&lt;li&gt;&lt;strong&gt;Pricing:&lt;/strong&gt; Enhancing privacy, reducing network dependency, and lowering development costs&lt;/li&gt;&#xA;&lt;/ul&gt;&#xA;&lt;h2 id=&#34;getting-started-with-ollama&#34;&gt;Getting Started with Ollama&lt;/h2&gt;&#xA;&lt;h3 id=&#34;download&#34;&gt;Download&lt;/h3&gt;&#xA;&lt;p&gt;&lt;strong&gt;Linux:&lt;/strong&gt;&lt;/p&gt;&#xA;&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;&#34;&gt;&lt;code class=&#34;language-sh&#34; data-lang=&#34;sh&#34;&gt;&lt;span style=&#34;display:flex;&#34;&gt;&lt;span&gt;curl -fsSL https://ollama.com/install.sh | sh&#xA;&lt;/span&gt;&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;strong&gt;Windows:&lt;/strong&gt; Visit the &lt;a href=&#34;https://ollama.com/download/windows&#34;&gt;Ollama download page&lt;/a&gt; and download the installer (&lt;code&gt;.exe&lt;/code&gt;).&#xA;&lt;strong&gt;MacOS&lt;/strong&gt; Visit the &lt;a href=&#34;https://ollama.com/download/mac&#34;&gt;Ollama download page&lt;/a&gt; and download the installer (&lt;code&gt;.app&lt;/code&gt;)&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
